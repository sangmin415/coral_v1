# AI-Assisted RF Capacitor Design Framework

This repository contains the source code and documentation for an AI-assisted framework that accelerates the inverse design of RF capacitors. We train machine-learning surrogate models on Keysight ADS (Momentum) simulation data so that Bayesian optimization can quickly suggest capacitor layouts without repeated EM solves.

The team will stage training jobs on a supercomputing cluster, so this README emphasises the division of responsibilities between the local development environment (data preparation, experiment orchestration, inverse design) and the remote high-performance computing (HPC) system (model training at scale).

---

## 1. Project Overview
<!-- í•œêµ­ì–´ ì£¼ì„: í”„ë¡œì íŠ¸ ê°œìš” -->

### 1.1 Goals
1.  **Reduce simulation turnaround** by replacing iterative ADS sweeps with an accurate Multi-Layer Perceptron (MLP) surrogate.
2.  **Optimize capacitor geometry** (currently limited to length `L` and width `W` per the PDK constraint) using Bayesian optimization.
3.  **Establish a reusable ML pipeline** that can be extended to inductors and RF filters once the capacitor workflow is validated.

### 1.2 Artifacts
-   **Source code** for preprocessing, training, and inverse design scripts.
-   **Configuration files** describing dataset schemas and experiment metadata.
-   **Documentation** covering data-readiness checks, HPC usage patterns, and future extensions.

---

## 2. End-to-End Workflow
<!-- í•œêµ­ì–´ ì£¼ì„: ì „ì²´ ì›Œí¬í”Œë¡œìš° -->

The design automation workflow is split into five stages with clear local â†” HPC boundaries.

| Stage | Owner | Description |
| --- | --- | --- |
| 1. **ADS ë°ì´í„° ìƒì„±** | RF ì„¤ê³„ íŒ€ | Keysight ADSì—ì„œ `L`, `W` íŒŒë¼ë¯¸í„° ìŠ¤ìœ•ì„ ìˆ˜í–‰í•´ Excel/CSV ë°ì´í„°ì…‹ì„ ìƒì„±í•©ë‹ˆë‹¤. ì¶”ê°€ íŒŒë¼ë¯¸í„°ëŠ” PDK ì œì•½ì— ë”°ë¼ ê³ ì •í•©ë‹ˆë‹¤. |
| 2. **ë°ì´í„° ìˆ˜ë ¹ ë° ê²€ì¦** | ë¡œì»¬ ê°œë°œ í™˜ê²½ | `docs/pre_data_readiness.md` ì²´í¬ë¦¬ìŠ¤íŠ¸ì— ë”°ë¼ ì—‘ì…€ ìŠ¤í‚¤ë§ˆ, ë‹¨ìœ„, ê²°ì¸¡ê°’ì„ í™•ì¸í•˜ê³  `configs/capacitor_baseline.yaml`ì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤. |
| 3. **ì „ì²˜ë¦¬ ë° íŒ¨í‚¤ì§•** | ë¡œì»¬ ê°œë°œ í™˜ê²½ | `python src/preprocess.py ...` ëª…ë ¹ìœ¼ë¡œ ì •ê·œí™”Â·ìŠ¤í”Œë¦¿ì„ ìˆ˜í–‰í•˜ê³ , ê²°ê³¼ë¬¼ì„ `data/processed/`ì— ì €ì¥í•©ë‹ˆë‹¤. |
| 4. **HPC ëª¨ë¸ í•™ìŠµ** | ìŠˆí¼ì»´í“¨í„° | ì „ì²˜ë¦¬ ì‚°ì¶œë¬¼ì„ ì „ì†¡ í›„ SLURM/LSF ë“± ìŠ¤ì¼€ì¤„ëŸ¬ì— í•™ìŠµ ì¡ì„ ì œì¶œí•©ë‹ˆë‹¤. í•™ìŠµ ë¡œê·¸ì™€ ê°€ì¤‘ì¹˜ë¥¼ `results/models/`ì— ì €ì¥í•©ë‹ˆë‹¤. |
| 5. **ì—­ì„¤ê³„ ë° ADS ì¬ê²€ì¦** | ë¡œì»¬ ê°œë°œ í™˜ê²½ | í•™ìŠµëœ ëª¨ë¸ì„ ì‚¬ìš©í•´ `python src/optimize.py ...` ë¡œ ìµœì ì˜ `L`, `W`ë¥¼ íƒìƒ‰í•˜ê³  ADSì—ì„œ ì¬ì‹œë®¬ë ˆì´ì…˜í•´ í™•ì¸í•©ë‹ˆë‹¤. |

---

## 3. Directory Structure
<!-- í•œêµ­ì–´ ì£¼ì„: í”„ë¡œì íŠ¸ í´ë” êµ¬ì¡° -->

```
.
â”œâ”€â”€ ads/                  # Keysight ADS project files and AEL scripts
â”œâ”€â”€ configs/              # YAML configs describing dataset schema & experiments
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/              # Raw data exported from ADS or lab measurements
â”‚   â””â”€â”€ processed/        # Normalized CSV/NPZ splits + scaler metadata
â”œâ”€â”€ docs/                 # Additional documentation (e.g., pre-data checklist)
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ models/           # Saved surrogate models (.pth)
â”‚   â””â”€â”€ plots/            # Validation plots and figures
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_utils.py     # Torch dataset + scaler helpers
â”‚   â”œâ”€â”€ preprocess.py     # Script for data preprocessing
â”‚   â”œâ”€â”€ train.py          # Script for training the MLP surrogate model
â”‚   â””â”€â”€ optimize.py       # Script for Bayesian Optimization
â”œâ”€â”€ requirements.txt      # Python dependencies
â””â”€â”€ README.md             # Project documentation
```

---

## 4. Environment Setup
<!-- í•œêµ­ì–´ ì£¼ì„: ì„¤ì¹˜ ë° í™˜ê²½ êµ¬ì„± -->

This project requires Python 3.9 or higher.

1.  Clone the repository:
    ```bash
    git clone https://github.com/your-username/ai-rf-capacitor-design.git
    cd ai-rf-capacitor-design
    ```

2.  Install the required Python packages using pip:
    ```bash
    pip install -r requirements.txt
    ```

The core dependencies are:
- `numpy`
- `pandas`
- `torch`
- `optuna`
- `matplotlib`
- `scikit-learn`

### 4.1 Local vs. HPC
-   **Local machine**: Run preprocessing, configuration edits, Bayesian optimization trials, and lightweight sanity checks.
-   **HPC cluster**: Execute large-scale training with the same Python environment. Use a virtual environment or module system to install `requirements.txt` on the cluster.

> ğŸ’¡ **Tip**: Use `pip freeze > requirements-lock.txt` on the HPC side to capture the exact versions that produced a model artifact.

---

## 5. Standard Operating Procedure
<!-- í•œêµ­ì–´ ì£¼ì„: ìš´ì˜ ì ˆì°¨ -->

The commands below outline the baseline experiment path. Replace placeholder paths with the actual Excel file name once received from the supervising professor.

### 1. Preprocess the Data (Local)
Run the preprocessing script locally to convert Excel/CSV exports into normalized PyTorch-ready tensors.
```bash
python src/preprocess.py \
  --input-path data/raw/ads_capacitor.xlsx \
  --config configs/capacitor_baseline.yaml \
  --output-dir data/processed/
```

### 2. Train the Surrogate Model (HPC)
Training occurs on the supercomputer. The example below assumes a SLURM scheduler; adapt it to your cluster environment.

1.  **Stage the dataset** to the HPC scratch directory (e.g., using `rsync`).
    ```bash
    rsync -avz data/processed/ user@hpc:/scratch/$USER/coral_v1/data/processed/
    ```

2.  **Submit a job script** (`scripts/train_capacitor.sbatch` example):
    ```bash
    #!/bin/bash
    #SBATCH --job-name=cap_train
    #SBATCH --time=04:00:00
    #SBATCH --partition=gpu
    #SBATCH --gres=gpu:1
    #SBATCH --cpus-per-task=8
    #SBATCH --mem=32G

    module load python/3.10 cuda/12.1  # í´ëŸ¬ìŠ¤í„° í™˜ê²½ì— ë§ê²Œ ìˆ˜ì •
    source ~/envs/coral/bin/activate

    cd $SCRATCH/coral_v1
    python src/train.py \
      --data-path data/processed/train.npz \
      --val-path data/processed/val.npz \
      --config configs/capacitor_baseline.yaml \
      --output-dir results/models/
    ```

3.  **Monitor and retrieve** results back to local storage when the job completes.
    ```bash
    rsync -avz user@hpc:/scratch/$USER/coral_v1/results/models/ results/models/
    ```

### 3. Run Bayesian Optimization (Local)
Back on the local machine, use the trained weights to search for target specifications (e.g., capacitance = 1.5 pF).
```bash
python src/optimize.py \
  --model-path results/models/mlp_surrogate.pth \
  --target-capacitance 1.5e-12 \
  --bounds "{'L': [10e-6, 120e-6], 'W': [4e-6, 40e-6]}"
```

### 4. Validate in ADS (RF Team)
Import the optimized `L`, `W` values into the ADS layout, run a final EM simulation, and compare the measured capacitance and Q-factor against the model prediction. Feed the validated result back into the dataset for continual improvement.

---

## 6. Future Work
<!-- í•œêµ­ì–´ ì£¼ì„: í–¥í›„ í™•ì¥ ë°©í–¥ -->

This framework can be extended in several promising directions:

-   **Advanced Surrogate Models:** Implement Convolutional Neural Networks (CNNs) or U-Nets for pixel-based capacitor layouts, allowing for more flexible and complex geometries once layout images become available.
-   **Multi-Platform Integration:** Integrate with other EM simulation tools like Ansys HFSS or Sonnet for broader applicability.
-   **Multi-Objective Optimization:** Extend the optimization goal to handle trade-offs between multiple parameters, such as maximizing the Q-factor while achieving a target capacitance (C-Q trade-off).
-   **Transfer Learning:** Apply transfer learning techniques to adapt existing surrogate models to new manufacturing processes or technology nodes with minimal retraining effort.

---

## 7. Reference Checklist
-   Review `docs/pre_data_readiness.md` before ingesting the professor's Excel dataset.
-   Confirm that the `configs/capacitor_baseline.yaml` file matches the actual column names and units.
-   Keep a changelog of HPC training runs (date, git commit, config hash, dataset version) for reproducibility.
